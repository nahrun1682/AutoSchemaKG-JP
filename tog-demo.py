#!/usr/bin/env python3
"""
Simple demo script that loads a graph generated by the KG pipeline,
builds OpenAI embeddings, and runs the TOG retriever for a single question.
"""
from __future__ import annotations

import argparse
import json
import os
from pathlib import Path
from typing import Iterable, List, Sequence

import networkx as nx
import numpy as np
from dotenv import load_dotenv
from openai import OpenAI

from tqdm.auto import tqdm

from atlas_rag.llm_generator import LLMGenerator
from atlas_rag.retriever.inference_config import InferenceConfig
from atlas_rag.retriever.tog import TogRetriever
from atlas_rag.vectorstore.embedding_model import EmbeddingAPI

class FixedLLMGenerator(LLMGenerator):
    def _api_inference(
        self,
        message,
        max_new_tokens=8192,
        temperature=0.7,
        frequency_penalty=None,
        response_format={"type": "text"},
        return_text_only=True,
        return_thinking=False,
        reasoning_effort=None,
        **kwargs,
    ):
        import time
        from openai import NOT_GIVEN

        start_time = time.time()
        response = self.client.chat.completions.create(
            model=self.model_name,
            messages=message,
            max_tokens=max_new_tokens,
            temperature=temperature,
            frequency_penalty=NOT_GIVEN if frequency_penalty is None else frequency_penalty,
            response_format=response_format if response_format is not None else {"type": "text"},
            timeout=120,
            reasoning_effort=NOT_GIVEN if reasoning_effort is None else reasoning_effort,
        )

        time_cost = time.time() - start_time
        content = response.choices[0].message.content

        if content is None and hasattr(response.choices[0].message, "reasoning_content"):
            content = response.choices[0].message.reasoning_content

        validate_function = kwargs.get("validate_function", None)
        content = validate_function(content, **kwargs) if validate_function else content

        if "</think>" in content and not return_thinking:
            content = content.split("</think>")[-1].strip()
        else:
            if (
                hasattr(response.choices[0].message, "reasoning_content")
                and response.choices[0].message.reasoning_content is not None
                and return_thinking
            ):
                content = "<think>" + response.choices[0].message.reasoning_content + "</think>" + content

        if return_text_only:
            return content
        else:
            completion_usage_dict = response.usage.model_dump()
            completion_usage_dict["time"] = time_cost
            return content, completion_usage_dict


def chunked(seq: Sequence[str], size: int) -> Iterable[Sequence[str]]:
    for i in range(0, len(seq), size):
        yield seq[i : i + size]


def ensure_embeddings(
    texts: List[str],
    cache_path: Path,
    order_path: Path,
    embedder: EmbeddingAPI,
    batch_size: int = 256,
    desc: str = "",
) -> np.ndarray:
    if cache_path.exists() and order_path.exists():
        stored_order = json.loads(order_path.read_text())
        if stored_order == texts:
            return np.load(cache_path)

    cache_path.parent.mkdir(parents=True, exist_ok=True)
    order_path.parent.mkdir(parents=True, exist_ok=True)

    all_embeddings = []
    progress = tqdm(total=len(texts), desc=desc or cache_path.stem, unit="item") if texts else None
    for batch in chunked(texts, batch_size):
        batch_embeddings = embedder.encode(batch)
        all_embeddings.append(batch_embeddings)
        if progress:
            progress.update(len(batch))
    if progress:
        progress.close()
    stacked = np.vstack(all_embeddings).astype(np.float32)

    np.save(cache_path, stacked)
    order_path.write_text(json.dumps(texts, ensure_ascii=False, indent=2))
    return stacked


def load_graph(graph_path: Path) -> nx.MultiDiGraph:
    if not graph_path.exists():
        raise FileNotFoundError(f"GraphML file not found: {graph_path}")
    return nx.read_graphml(graph_path.as_posix())


def maybe_truncate(text: str, max_chars: int | None) -> str:
    if not isinstance(text, str):
        text = str(text)
    if not max_chars or max_chars <= 0:
        return text
    if len(text) <= max_chars:
        return text
    return text[:max_chars]


def prepare_embeddings(
    graph: nx.MultiDiGraph,
    embedder: EmbeddingAPI,
    cache_dir: Path,
    batch_size: int,
    text_max_chars: int | None,
) -> tuple[np.ndarray, np.ndarray]:
    node_keys = list(graph.nodes())
    node_texts = [
        maybe_truncate(graph.nodes[k].get("id", k), text_max_chars) for k in node_keys
    ]

    node_emb_path = cache_dir / "tog_node_embeddings.npy"
    node_order_path = cache_dir / "tog_node_order.json"
    node_embeddings = ensure_embeddings(
        node_texts,
        node_emb_path,
        node_order_path,
        embedder,
        batch_size=batch_size,
        desc="ãƒãƒ¼ãƒ‰åŸ‹ã‚è¾¼ã¿",
    )

    edge_keys = list(graph.edges())
    edge_texts = []
    for head, tail in edge_keys:
        head_text = maybe_truncate(graph.nodes[head].get("id", head), text_max_chars)
        tail_text = maybe_truncate(graph.nodes[tail].get("id", tail), text_max_chars)
        relation = graph.edges[(head, tail)].get("relation", "related_to")
        edge_texts.append(f"{head_text} {relation} {tail_text}")

    edge_emb_path = cache_dir / "tog_edge_embeddings.npy"
    edge_order_path = cache_dir / "tog_edge_order.json"
    edge_embeddings = ensure_embeddings(
        edge_texts,
        edge_emb_path,
        edge_order_path,
        embedder,
        batch_size=batch_size,
        desc="ã‚¨ãƒƒã‚¸åŸ‹ã‚è¾¼ã¿",
    )
    return node_embeddings, edge_embeddings


def answer_with_llm(llm: LLMGenerator, question: str, triples: List[str]) -> str:
    knowledge = "\n".join(triples) if triples else "è©²å½“ã™ã‚‹ãƒˆãƒªãƒ—ãƒ«ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"
    messages = [
        {
            "role": "system",
            "content": "ã‚ãªãŸã¯çŸ¥è­˜ã‚°ãƒ©ãƒ•ã®ãƒˆãƒªãƒ—ãƒ«ã‚’æ ¹æ‹ ã«å›ç­”ã™ã‚‹æ—¥æœ¬èªã®ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚"
                       "æ ¹æ‹ ãŒä¸è¶³ã—ã¦ã„ã‚‹å ´åˆã¯ãã®æ—¨ã‚’ä¼ãˆã¦ãã ã•ã„ã€‚",
        },
        {
            "role": "user",
            "content": f"è³ªå•: {question}\n"
                       f"åˆ©ç”¨å¯èƒ½ãªãƒˆãƒªãƒ—ãƒ«:\n{knowledge}\n\n"
                       "ã“ã‚Œã‚‰ã®æƒ…å ±ã®ã¿ã‚’æ ¹æ‹ ã«ã€è©³ç´°ã«å›ç­”ã—ã¦ãã ã•ã„ã€‚",
        },
    ]
    return llm.generate_response(messages, max_new_tokens=512, temperature=0.2)


def main() -> None:
    parser = argparse.ArgumentParser(description="Run TOG retrieval on a question.")
    parser.add_argument(
        "--question",
        default="æ—¥æœ¬ã«æµé€šã—ã¦ã‚‹æµ·ç”£ç‰©ã¯ï¼Ÿ",
        help="å•ã„åˆã‚ã›ã‚‹è³ªå•ï¼ˆæ—¥æœ¬èªæ¨å¥¨ï¼‰",
    )
    parser.add_argument(
        "--graph",
        default="import/niconico_2024_month/kg_graphml/niconico_2024_month_graph.graphml",
        help="GraphML ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹",
    )
    parser.add_argument(
        "--cache_dir",
        default="import/niconico_2024_month/vector_index",
        help="åŸ‹ã‚è¾¼ã¿ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã™ã‚‹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª",
    )
    parser.add_argument(
        "--model",
        default="gpt-4o-mini",
        help="è³ªå•å¿œç­”ã«ä½¿ç”¨ã™ã‚‹OpenAIãƒãƒ£ãƒƒãƒˆãƒ¢ãƒ‡ãƒ«å",
    )
    parser.add_argument(
        "--embedding_model",
        default="text-embedding-3-small",
        help="ãƒãƒ¼ãƒ‰ï¼ã‚¨ãƒƒã‚¸åŸ‹ã‚è¾¼ã¿ã«ä½¿ç”¨ã™ã‚‹OpenAIåŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«å",
    )
    parser.add_argument(
        "--embed_batch_size",
        type=int,
        default=64,
        help="åŸ‹ã‚è¾¼ã¿APIã«æ¸¡ã™1ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚ãŸã‚Šã®ä»¶æ•°",
    )
    parser.add_argument(
        "--text_max_chars",
        type=int,
        default=1024,
        help="ãƒãƒ¼ãƒ‰/ã‚¨ãƒƒã‚¸ãƒ†ã‚­ã‚¹ãƒˆã‚’å…ˆé ­ã‹ã‚‰ã“ã®æ–‡å­—æ•°ã«åˆ¶é™ï¼ˆ0ä»¥ä¸‹ã§ç„¡åˆ¶é™ï¼‰",
    )
    args = parser.parse_args()

    load_dotenv()
    if not os.getenv("OPENAI_API_KEY"):
        raise RuntimeError("OPENAI_API_KEY ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ (.env ã‚’ç¢ºèªã—ã¦ãã ã•ã„)ã€‚")

    graph_path = Path(args.graph)
    cache_dir = Path(args.cache_dir)

    print("ğŸ“„ ã‚°ãƒ©ãƒ•ã‚’èª­ã¿è¾¼ã¿ä¸­...")
    graph = load_graph(graph_path)
    print(f"   ãƒãƒ¼ãƒ‰æ•°: {graph.number_of_nodes()}, ã‚¨ãƒƒã‚¸æ•°: {graph.number_of_edges()}")

    client = OpenAI()
    llm = FixedLLMGenerator(client, model_name=args.model)
    embedder = EmbeddingAPI(client, model_name=args.embedding_model)

    print("ğŸ§® ãƒãƒ¼ãƒ‰ï¼ã‚¨ãƒƒã‚¸åŸ‹ã‚è¾¼ã¿ã‚’æº–å‚™ã—ã¦ã„ã¾ã™...")
    node_embeddings, edge_embeddings = prepare_embeddings(
        graph,
        embedder,
        cache_dir,
        args.embed_batch_size,
        args.text_max_chars,
    )
    print(f"   ãƒãƒ¼ãƒ‰åŸ‹ã‚è¾¼ã¿å½¢çŠ¶: {node_embeddings.shape}, ã‚¨ãƒƒã‚¸åŸ‹ã‚è¾¼ã¿å½¢çŠ¶: {edge_embeddings.shape}")

    retriever = TogRetriever(
        llm_generator=llm,
        sentence_encoder=embedder,
        data={"KG": graph, "node_embeddings": node_embeddings, "edge_embeddings": edge_embeddings},
        inference_config=InferenceConfig(Dmax=5, topk=5),
    )

    print(f"â“ è³ªå•: {args.question}")
    triples, _ = retriever.retrieve(args.question, topN=5)
    print("ğŸ“š å–å¾—ã—ãŸãƒˆãƒªãƒ—ãƒ«:")
    for triple in triples:
        print(f"  - {triple}")

    answer = answer_with_llm(llm, args.question, triples)
    print("\nğŸ’¡ å›ç­”:")
    print(answer.strip())


if __name__ == "__main__":
    main()
